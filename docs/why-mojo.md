# 为何是 Mojo🔥

当我们创立 Modular 时，我们本无意构建一种新的编程语言。但是当我们构建 [面向 ML/AI 基础设施的统一平台](https://www.modular.com/blog/the-case-for-a-next-generation-ai-developer-platform) 时，我们意识到整个技术栈的编程过于复杂。同时，我们手写了大量 MLIR，整个过程很费力。

我们需要的是一种创新且可扩展的编程模型，用于面向加速器（accelerators）和 AI 领域普遍存在的若干异构系统。这意味着编程语言需要具有强大的编译时元编程、集成自适应编译技术、缓存整个编译流程以及现有编程语言不支持的其他特性。

尽管加速器很重要，但最普遍且有时被忽视的“加速器”之一是主机 CPU。如今，CPU 拥有许多类似张量核（tensor-core-like）的加速器模块和其他 AI 加速单元，同时，它们也用于专用加速器无法处理的场景（例如：数据加载，预处理/后处理以及与外部系统的集成），因此，我们不能仅用一种仅适配特定处理器的“加速器语言”来提升 AI 性能。

面对 AI 应用场景需要解决所有这些问题，我们认为没有理由不能只用一种语言来完成。因此，Mojo 诞生了。

## 采用下一代编译器技术的语言

当我们意识到现有的编程语言无法解决 AI 计算的挑战时，我们开始重新思考如何设计和实现编程语言来解决我们的问题。由于我们需要支持各种加速器的高性能特性，而传统的编译器技术（如 LLVM 和 GCC）并不适合此类场景（任何基于它们的语言和工具也无法满足）。尽管它们支持各种 CPU 和一些常用的 GPU，但是这些编译器技术设计于几十年前，其无法全面支持现代芯片架构。现如今，专用机器学习加速器的标准技术则是 MLIR。

MLIR 是一个相对较新的开源编译器基础设施，其开始于 Google（目前已转移至 Modular），目前整个机器学习加速器社区中已广泛采用 MLIR。MLIR 优势在于它能够构建特定领域的编译器，特别是对于非传统 CPU 和 GPU 的特殊领域，例如：AI ASICS，量子计算系统，FPGA 和定制化芯片。

鉴于我们在 Modular 的目标是构建下一代 AI 平台，我们已经将 MLIR 应用于我们的一些基础设施，但我们没有一种编程语言可以完全释放 MLIR 在整个技术栈中的全部潜力。虽然许多其他项目现已使用 MLIR，但是 Mojo 是第一个专门为 MLIR 设计的重要编程语言，这使得 Mojo 足以胜任 AI 任务场景下编写系统级代码。

## 拥抱 Python 生态

TBD

### 我们为何选择 Python

TBD

## 兼容 Python

TBD

### 与 Python 的差异（有意为之）

TBD

## Python 的问题

TBD

### two-world 问题

TBD

### three-world 和 N-world 问题

TBD

### 移动端与服务端部署

TBD

## 相关工作

TBD

### 改进 CPython 和 JIT 编译时 Python

TBD

### Python 子集和其他类似 Python 的语言

TBD

### 与 C 兼容的 Python 超集

TBD

### Python 嵌入式 DSL

TBD